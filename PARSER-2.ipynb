{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "332ef9dd",
   "metadata": {},
   "source": [
    "Парсим исторические сведения о погоде с июля 2017 года по апрель 2023 с сайта https://pogoda1.ru/ для Москвы (точнее - метеостанции Внуково) и Санкт-Петербурга (точнее - метеостанции Пулково).\n",
    "\n",
    "Очень приятный сайт, для которого не нужно заморачиваться с fake useragent и запросами через прокси. \n",
    "\n",
    "Нужные страницы лежат под простой маской сайт/место/дата, что позволяет обернуть весь процесс парсинга в приятный цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3566cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f1800b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем временные индексы для цикла \n",
    "dates = pd.date_range(start='7/6/2017', end='30/4/2023')\n",
    "dates = pd.Series(dates)\n",
    "\n",
    "# переменные для удобства\n",
    "moscow = 'moscow-vnukovo-airport'\n",
    "spb = 'sankt-peterburg-pulkovo-airport'\n",
    "\n",
    "# интересующие нас признаки и датафреймы для каждого города\n",
    "cols = ['Ночная температура', 'Дневная температура', 'Влажность', 'Давление', \n",
    "           'Направление ветра', 'Сила ветра', 'Погодные явления']\n",
    "\n",
    "df_moscow = pd.DataFrame(columns=cols, index=dates)\n",
    "df_spb = pd.DataFrame(columns=cols, index=dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d0cbf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пишем мегафункцию для парсинга \n",
    "# изначально было без try except, после ~7 ошибок из-за неравномерной структуры сайта\n",
    "# добавил эти конструкции и парсинг прошел без проблем\n",
    "# в конце есть time.sleep() на всякий случай, чтобы не перегружать сервер запросами\n",
    "\n",
    "def get_values(date, place, df): \n",
    "    \n",
    "    \"\"\"\n",
    "    Функция получает date (дату наблюдений), place (место наблюдений), \n",
    "    берет их с сайта pogoda1.ru и записывает в массиве df, где должен быть соответствующей date индекс.\n",
    "    Ничего не возвращает.\n",
    "    \n",
    "    Записываемые наблюдения: 'Ночная температура', 'Дневная температура', 'Влажность', 'Давление', \n",
    "                           'Направление ветра', 'Сила ветра', 'Погодные явления'\n",
    "                           \n",
    "    Параметры:                      \n",
    "    date - дата в datetime\n",
    "    place - Внуково/Пулково, здесь задаются переменными moscow/spb\n",
    "    df - массив для сохранения, здесь задаются переменными df_moscow/df_spb\n",
    "    \"\"\"\n",
    "\n",
    "    y = date.strftime('%d-%m-%Y') # переводим дату в строку в нужном формате, чтобы подставлять в адрес \n",
    "    page_link = f'https://pogoda1.ru/{place}/{y}/' # подставляем в адрес \n",
    "    response = requests.get(page_link)\n",
    "    if response.ok == True: # проверяем, пустил ли сервер на страничку; если все ок - извлекаем из html нужные данные\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # четыре контейнера obj, гле лежат нужные данные; не очень красивые преобразования строк \n",
    "        obj1 = soup.findAll(attrs={'class': 'cell-forecast-temp'})\n",
    "        try:\n",
    "            temp_night, temp_day = int(obj1[1].contents[0][:-1].strip()), int(obj1[3].contents[0][:-1].strip())\n",
    "        except:\n",
    "            temp_night, temp_day = np.nan, np.nan\n",
    "        \n",
    "        obj2 = soup.findAll(attrs={'class': 'weather-now-info'})\n",
    "        humidity = int(obj2[1].contents[3].contents[0][:-1].strip())\n",
    "        pressure = int(obj2[0].contents[3].contents[0][:3].strip())\n",
    "        \n",
    "        obj3 = soup.findAll(attrs={'class': 'wind-amount'})\n",
    "        try:\n",
    "            wind_direction = obj3[0].contents[0][:-4].split(',')[0]\n",
    "            wind_velocity = int(obj3[0].contents[0][:-4].split(',')[1].strip())\n",
    "        except:\n",
    "            wind_direction, wind_velocity = np.nan, np.nan\n",
    "        \n",
    "        obj4 = soup.findAll(attrs={'class': 'weather-now-icon-img'})\n",
    "        weather_cond = str(obj4[0]).split(' ')[-1].split('\"')[1]\n",
    "        \n",
    "        # разобрались с контейнерами, теперь записываем в массив\n",
    "        df.loc[date] = [temp_night, temp_day, humidity, pressure, wind_direction, wind_velocity, weather_cond]\n",
    "        \n",
    "    else: # для всех ошибок доступа, но практически только для 404 - когда нет данных за день\n",
    "        df.loc[date] = np.full(7, np.nan)\n",
    "        print(response)\n",
    "        print(date)\n",
    "    \n",
    "    time.sleep(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "70aaca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем гуся и сохраняем результаты в csv-файлы (с соответствующми названиями будут на гитхабе); \n",
    "# индекс сохраняется в колонку, но это некритично\n",
    "# этап с парсингом завершен\n",
    "\n",
    "for date in dates:\n",
    "    get_values(date, spb, df_spb)\n",
    "df_spb.to_csv('weather_spb_2017_2023')\n",
    "\n",
    "for date in dates:\n",
    "    get_values(date, moscow, df_moscow)\n",
    "df_moscow.to_csv('weather_moscow_2017_2023')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
